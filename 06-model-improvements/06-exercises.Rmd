<<<<<<< HEAD
---
title: "06-exercises"
author: "Ali Marami"
date: "2016-05-18"
output: html_document
---

## Reading:
- **APM** Chapter 8.6 and 8.8 
- **APM** Chapter 14.8 
- **APM** Chapter 7.1 & 7.3 "Non-Linear Regression Models"
- **APM** Chapter 13.2 & 13.4 "Non-Linear Classifcation Models"


```{r,echo=FALSE}

packs <-  c('AppliedPredictiveModeling', 'ggplot2', 'magrittr', 'dplyr', 'caret')

for( nm in packs ) { 
  # message(nm)
  if( ! nm  %in% installed.packages()[,1]  ) install.packages(nm)
  library(nm, character.only = TRUE)
}

# Load data set into environment
data(FuelEconomy)
.. = NULL  # Needed for aesthetics 

FE <- dplyr::bind_rows(cars2010, cars2011, cars2012)    # Define Da

```

## Fuel Economy 


This week we return to the Fuel Economy Data having learned much about model building. This assignment is to go through the process of building several regression models and pick the most predictive model. Use the `FE` data set created for you above.


Start by making choosing a metric and making a naive guess of model performance: 

Metric: _______rmse________
Naive Guess: ______34.6_________
Expected Model Performance (based on Naive Guess): ______8.1_________

Show your work below for the calculations

```{r} 

  
naive_guess = median(FE$FE)

err_naive_guess = (FE$FE-rep(34.6, length(FE$FE)))^2 %>% mean %>% sqrt

```


Based only your intuition, how low do your think you can get your metric: _____8.1___because it's the median___


## Examine your data

 * Plot your response/outcome 

 * Make a guess of a strong predictor: ____NumCyl_____________  
 * Plot your response vs your predictor. 

```{r}



plot(FE$FE,predict(lm(FE$FE ~ FE$EngDispl)))
FE %>% ggplot(aes(x=EngDispl, y=FE))+geom_point()+stat_smooth(method="lm", se=FALSE)


```



## Build Simple Models

Using **caret**, build a simple linear model and a simple tree model. 

```{r}
rmse <- function(y,yhat) {

  (y-yhat)^2 %>% mean %>% sqrt
}

ctrl <- trainControl( method="boot", number=5
                      , classProb=TRUE, savePrediction=TRUE )

fit.lm <- fit.lm <- train(FE ~ ., data=FE, method="lm", trControl=ctrl)
fit.rp <- train( FE ~ ., data=FE, trControl=ctrl, method="rpart", tuneLength=20) 



```


What did you learn about the data from these models. EngDispl and NumCyl are the main predictors



## Build More Advanced Models

Now refine your models. Use **caret** to build advanced models:
- one that uses model averaging (bagging) 
- one that uses boosting 

```{r}

# Your work here.

fit.bag <- bag(FE, FE$FE, B = 10,
               bagControl = bagControl(fit = ctreeBag$fit,
                                       predict = ctreeBag$pred,
                                       aggregate = ctreeBag$aggregate))


fit.boost <- train( FE ~ ., data=FE, trControl=ctrl
              , method="gbm")


```


## Conclusion 

Which model would you use and why?  Under different circumstances why would you choose one of the other models.

I choose fit.bag since it has the best performance. For fit.boost, I didn't get the ValidDeviance and could not sort it out. Probably for larger datasets for which I need more computation power, I choose simpler models even with lower performance to reduce the cost of computation.


=======
---
title: "05-exercises"
author: "Your Name Here"
date: "2016-05-xx"
output: html_document
---

## Reading:
- **APM** Chapter 8.6 and 8.8 
- **APM** Chapter 14.8 
- **APM** Chapter 7.1 & 7.3 "Non-Linear Regression Models"
- **APM** Chapter 13.2 & 13.4 "Non-Linear Classifcation Models"


```{r,echo=FALSE}

packs <-  c('AppliedPredictiveModeling', 'ggplot2', 'magrittr', 'dplyr')

for( nm in packs ) { 
  # message(nm)
  if( ! nm  %in% installed.packages()[,1]  ) install.packages(nm)
  library(nm, character.only = TRUE)
}

# Load data set into environment
data(FuelEconomy)
.. = NULL  # Needed for aesthetics 

FE <- dplyr::bind_rows(cars2010, cars2011, cars2012)    # Define Da

```

## Fuel Economy 


This week we return to the Fuel Economy Data having learned much about model building. This assignment is to go through the process of building several regression models and pick the most predictive model. Use the `FE` data set created for you above.


Start by making choosing a metric and making a naive guess of model performance: 

Metric: _______________
Naive Guess: _______________
Expected Model Performance (based on Naive Guess): _______________

Show your work below for the calculations

```{r} 

  
naive_guess = ..

err_naive_guess = ..

```


Based only your intuition, how low do your think you can get your metric: ___________


## Examine your data

 * Plot your response/outcome 

 * Make a guess of a strong predictor: _________________  
 * Plot your response vs your predictor. 

```{r}

.. # Your work here

```



## Build Simple Models

Using **caret**, build a simple linear model and a simple tree model. 

```{r}

fit.lm <- .. # Your model here
fit.rp <- .. # Your model here

```


What did you learn about the data from these models.



## Build More Advanced Models

Now refine your models. Use **caret** to build advanced models:
- one that uses model averaging (bagging) 
- one that uses boosting 

```{r}

# Your work here.
fit.bag   <- .. # bagging model 

fit.boost <- .. # boosting model


```


## Conclusion 

Which model would you use and why?  Under different circumstances why would you choose one of the other models.

>>>>>>> 5bb4d32cc193c9fdb60b0631f0845b412030115e
